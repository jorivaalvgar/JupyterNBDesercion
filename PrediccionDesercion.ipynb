{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Deserción Estudiantil con Redes Neuronales\n",
    "\n",
    "Este notebook permite predecir la deserción estudiantil en Instituciones de Educación Superior (IES).\n",
    "Se basa en 4 variables, pero puede ser expandido para usar las que la institución considere necesarias, siempre y cuando haga una estandarización de los grupos de datos(1 o 0 por columnas).\n",
    "Las dimensiones del dataset de ejemplo son:\n",
    "\n",
    "- Económica: 6 niveles. Basado en la agregación de las variables ingreso_familiar, estudiante_trabaja, estrato, nivel sisven, apoyo_icetex, y estandarizado mediante divisiones para llegar a los 6 niveles.\n",
    "- Personal: 5 niveles. Basado en la agregación de las variables: padre_lee, madre_lee, familia_personas, familia_hermanos, nivel_educacion_madre, nivel_educacion_padre, estudiante_genero\n",
    "- Institucional: 5 niveles. Basado en la agregación de las columnas: programa_metodologia, programa_area, programa_nivel\n",
    "- Académica: 6 niveles. Basado en la sumatoria de los puntajes SABER del estudiante, debe adicionarse o reemplazarse con las calificaciones del estudiante.\n",
    "\n",
    "El dataset original está basado en la base SPADIES de 2016 pero es solamente para propósito de pruebas, y se espera que las IES determinen los valores para estas dimensiones o establezcan nuevas. Los valores fueron filtrados para tener un dataset de información relevante.\n",
    "\n",
    "Para cargar los dataset se usan dos librerías, Pandas y numpy, sobre los cuales se puede encontrar documentación en estos sitios de referencia:\n",
    "- https://pandas.pydata.org/pandas-docs/stable/\n",
    "- https://numpy.org/doc/\n",
    "Para las redes neuronales se usaron un algoritmo Adaline sencillo monocapa, escrito en código, y la librería scikit de la cual se pueden encontrar ejemplos de uso en esta página:\n",
    "- https://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "Se pueden usar las implementaciones de ejemplo de cada algoritmo y con la configuración de los hiperparámetros se puede determinar el algoritmo más conveniente y preciso.\n",
    "Los algoritmos implementados aquí son: Adaline, Perceptrón multicapa, Perceptrón regresor multicapa y Random Forest\n",
    "\n",
    "\n",
    "## Importante\n",
    "Las librerías pandas, numpy y scikit deben ser instaladas en el ambiente del notebook para que funcione. \n",
    "Se sugiere usar pip para hacer la instalación mediante el comando: 'pip install \"librería\"', por ejemplo: pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>academica</th>\n",
       "      <th>personal</th>\n",
       "      <th>economica</th>\n",
       "      <th>institucional</th>\n",
       "      <th>deserta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   academica  personal  economica  institucional  deserta\n",
       "1          2         1          2              2        0\n",
       "2          2         1          2              2        0\n",
       "3          3         1          4              2        0\n",
       "4          2         2          2              2        1\n",
       "5          3         1          2              2        0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando pandas y numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# leyendo el dataset\n",
    "data = pd.read_csv('Datos_estudiantes.csv')\n",
    "\n",
    "# Valida la carga de datos\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional\n",
    "## Dibujando los datos\n",
    "\n",
    "Vamos a dibujar la data para ver cómo luce, vamos a tomar dos dimensiones originalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_points(data):\n",
    "    X = np.array(data[[\"economica\",\"academica\"]])\n",
    "    y = np.array(data[\"deserta\"])\n",
    "    desertor = X[np.argwhere(y==1)]\n",
    "    continua = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in continua], [s[0][1] for s in continua], s = 25, color = 'red', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in desertor], [s[0][1] for s in desertor], s = 25, color = 'cyan', edgecolor = 'k')\n",
    "    plt.xlabel('D. Economica')\n",
    "    plt.ylabel('D. Familiar')\n",
    "    \n",
    "plot_points(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente no son fácilmente separables los grupos de desertores de los que continuan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar los datos en binario\n",
    "\n",
    "Una vez que tenemos un rango de valores, por ejemplo Institucional, con valores 0 a 4\n",
    "Use la función `get_dummies` de pandas para codificar en valores 0 o 1 en columnas correspondientes a los posibles valores de la dimensión. \n",
    "\n",
    "Finalmente, elimine la columna codificada usando la función `drop`. \n",
    "Para más información:(https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deserta</th>\n",
       "      <th>academica_0</th>\n",
       "      <th>academica_1</th>\n",
       "      <th>academica_2</th>\n",
       "      <th>academica_3</th>\n",
       "      <th>academica_4</th>\n",
       "      <th>academica_5</th>\n",
       "      <th>personal_0</th>\n",
       "      <th>personal_1</th>\n",
       "      <th>personal_2</th>\n",
       "      <th>...</th>\n",
       "      <th>institucional_1</th>\n",
       "      <th>institucional_2</th>\n",
       "      <th>institucional_3</th>\n",
       "      <th>institucional_4</th>\n",
       "      <th>economica_0</th>\n",
       "      <th>economica_1</th>\n",
       "      <th>economica_2</th>\n",
       "      <th>economica_3</th>\n",
       "      <th>economica_4</th>\n",
       "      <th>economica_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    deserta  academica_0  academica_1  academica_2  academica_3  academica_4  \\\n",
       "1         0            0            0            1            0            0   \n",
       "2         0            0            0            1            0            0   \n",
       "3         0            0            0            0            1            0   \n",
       "4         1            0            0            1            0            0   \n",
       "5         0            0            0            0            1            0   \n",
       "6         1            0            0            1            0            0   \n",
       "7         0            0            0            1            0            0   \n",
       "8         0            0            0            1            0            0   \n",
       "9         0            0            0            1            0            0   \n",
       "10        0            0            0            1            0            0   \n",
       "\n",
       "    academica_5  personal_0  personal_1  personal_2  ...  institucional_1  \\\n",
       "1             0           0           1           0  ...                0   \n",
       "2             0           0           1           0  ...                0   \n",
       "3             0           0           1           0  ...                0   \n",
       "4             0           0           0           1  ...                0   \n",
       "5             0           0           1           0  ...                0   \n",
       "6             0           0           1           0  ...                0   \n",
       "7             0           0           1           0  ...                0   \n",
       "8             0           0           1           0  ...                0   \n",
       "9             0           0           1           0  ...                0   \n",
       "10            0           0           1           0  ...                0   \n",
       "\n",
       "    institucional_2  institucional_3  institucional_4  economica_0  \\\n",
       "1                 1                0                0            0   \n",
       "2                 1                0                0            0   \n",
       "3                 1                0                0            0   \n",
       "4                 1                0                0            0   \n",
       "5                 1                0                0            0   \n",
       "6                 0                1                0            1   \n",
       "7                 1                0                0            0   \n",
       "8                 1                0                0            0   \n",
       "9                 0                1                0            0   \n",
       "10                1                0                0            0   \n",
       "\n",
       "    economica_1  economica_2  economica_3  economica_4  economica_5  \n",
       "1             0            1            0            0            0  \n",
       "2             0            1            0            0            0  \n",
       "3             0            0            0            1            0  \n",
       "4             0            1            0            0            0  \n",
       "5             0            1            0            0            0  \n",
       "6             0            0            0            0            0  \n",
       "7             0            1            0            0            0  \n",
       "8             0            1            0            0            0  \n",
       "9             0            1            0            0            0  \n",
       "10            0            1            0            0            0  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot_data va a tener la data codificada\n",
    "one_hot_data = data[:]\n",
    "\n",
    "# crear columnas codificadas para academica y eliminar la columna original\n",
    "one_hot_data = pd.concat( [one_hot_data, pd.get_dummies(one_hot_data['academica'], prefix = 'academica')], axis=1)\n",
    "one_hot_data.drop([\"academica\"], axis=1, inplace=True)\n",
    "\n",
    "# crear columnas codificadas para personal y eliminar la columna original\n",
    "one_hot_data = pd.concat( [one_hot_data, pd.get_dummies(one_hot_data['personal'], prefix = 'personal')], axis=1)\n",
    "one_hot_data.drop([\"personal\"], axis=1, inplace=True)\n",
    "\n",
    "# crear columnas codificadas para institucional y eliminar la columna original\n",
    "one_hot_data = pd.concat( [one_hot_data, pd.get_dummies(one_hot_data['institucional'], prefix = 'institucional')], axis=1)\n",
    "one_hot_data.drop([\"institucional\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# crear columnas codificadas para personal y eliminar la columna original\n",
    "one_hot_data = pd.concat( [one_hot_data, pd.get_dummies(one_hot_data['economica'], prefix = 'economica')], axis=1)\n",
    "one_hot_data.drop([\"economica\"], axis=1, inplace=True)\n",
    "\n",
    "# verificar la data\n",
    "one_hot_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalar los datos\n",
    "\n",
    "El siguiente paso es escalar los datos. En este dataset ya se ha hecho este proceso dividiendo los valores hasta obtener unos rangos, pero si tiene data con valores continuos, debe buscar dejarlos como escalares. \n",
    "Por ejemplo: academica tenía valoes que oscilaban entre 1300 y 11500, al dividir entre 2000 y convertirlo a entero se logró el escalamiento:\n",
    "`processed_data[\"academica\"] = np.round(processed_data[\"academica\"]/2000).astype(int)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deserta</th>\n",
       "      <th>academica_0</th>\n",
       "      <th>academica_1</th>\n",
       "      <th>academica_2</th>\n",
       "      <th>academica_3</th>\n",
       "      <th>academica_4</th>\n",
       "      <th>academica_5</th>\n",
       "      <th>personal_0</th>\n",
       "      <th>personal_1</th>\n",
       "      <th>personal_2</th>\n",
       "      <th>...</th>\n",
       "      <th>institucional_1</th>\n",
       "      <th>institucional_2</th>\n",
       "      <th>institucional_3</th>\n",
       "      <th>institucional_4</th>\n",
       "      <th>economica_0</th>\n",
       "      <th>economica_1</th>\n",
       "      <th>economica_2</th>\n",
       "      <th>economica_3</th>\n",
       "      <th>economica_4</th>\n",
       "      <th>economica_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161025</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102195</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43601</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170362</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114716</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164121</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114502</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128964</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37409</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        deserta  academica_0  academica_1  academica_2  academica_3  \\\n",
       "161025        1            0            0            1            0   \n",
       "14636         1            0            0            1            0   \n",
       "102195        1            0            0            1            0   \n",
       "151856        0            0            0            1            0   \n",
       "3244          1            0            0            1            0   \n",
       "43601         0            0            0            1            0   \n",
       "170362        0            0            0            1            0   \n",
       "175388        0            0            0            0            1   \n",
       "114716        0            0            0            1            0   \n",
       "147941        0            0            0            1            0   \n",
       "51103         0            0            0            1            0   \n",
       "164121        1            0            0            0            1   \n",
       "114502        1            0            0            1            0   \n",
       "128964        0            0            0            1            0   \n",
       "37409         1            0            0            0            1   \n",
       "\n",
       "        academica_4  academica_5  personal_0  personal_1  personal_2  ...  \\\n",
       "161025            0            0           0           1           0  ...   \n",
       "14636             0            0           0           1           0  ...   \n",
       "102195            0            0           0           0           1  ...   \n",
       "151856            0            0           0           1           0  ...   \n",
       "3244              0            0           0           1           0  ...   \n",
       "43601             0            0           0           0           1  ...   \n",
       "170362            0            0           0           1           0  ...   \n",
       "175388            0            0           0           1           0  ...   \n",
       "114716            0            0           0           1           0  ...   \n",
       "147941            0            0           0           1           0  ...   \n",
       "51103             0            0           0           1           0  ...   \n",
       "164121            0            0           0           1           0  ...   \n",
       "114502            0            0           0           1           0  ...   \n",
       "128964            0            0           0           0           1  ...   \n",
       "37409             0            0           0           1           0  ...   \n",
       "\n",
       "        institucional_1  institucional_2  institucional_3  institucional_4  \\\n",
       "161025                0                1                0                0   \n",
       "14636                 0                1                0                0   \n",
       "102195                0                0                1                0   \n",
       "151856                0                1                0                0   \n",
       "3244                  0                1                0                0   \n",
       "43601                 0                1                0                0   \n",
       "170362                0                0                1                0   \n",
       "175388                0                0                1                0   \n",
       "114716                0                0                1                0   \n",
       "147941                0                1                0                0   \n",
       "51103                 0                1                0                0   \n",
       "164121                0                0                1                0   \n",
       "114502                0                0                1                0   \n",
       "128964                0                1                0                0   \n",
       "37409                 0                1                0                0   \n",
       "\n",
       "        economica_0  economica_1  economica_2  economica_3  economica_4  \\\n",
       "161025            0            1            0            0            0   \n",
       "14636             1            0            0            0            0   \n",
       "102195            0            0            1            0            0   \n",
       "151856            0            1            0            0            0   \n",
       "3244              0            0            1            0            0   \n",
       "43601             0            0            1            0            0   \n",
       "170362            0            1            0            0            0   \n",
       "175388            0            0            1            0            0   \n",
       "114716            0            0            1            0            0   \n",
       "147941            0            0            1            0            0   \n",
       "51103             0            0            1            0            0   \n",
       "164121            1            0            0            0            0   \n",
       "114502            0            0            1            0            0   \n",
       "128964            0            1            0            0            0   \n",
       "37409             0            0            1            0            0   \n",
       "\n",
       "        economica_5  \n",
       "161025            0  \n",
       "14636             0  \n",
       "102195            0  \n",
       "151856            0  \n",
       "3244              0  \n",
       "43601             0  \n",
       "170362            0  \n",
       "175388            0  \n",
       "114716            0  \n",
       "147941            0  \n",
       "51103             0  \n",
       "164121            0  \n",
       "114502            0  \n",
       "128964            0  \n",
       "37409             0  \n",
       "\n",
       "[15 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se copia en este caso, un porcentaje aleatorio del dataset original por ser un dataset grande (>200.000 registros)\n",
    "sample = np.random.choice(one_hot_data.index, size=int(len(processed_data)*0.2), replace=False)\n",
    "processed_data = one_hot_data.iloc[sample]\n",
    "\n",
    "\n",
    "#processed_data[\"academica\"] = np.round(processed_data[\"academica\"]/2000).astype(int)\n",
    "\n",
    "# validando los resultados\n",
    "processed_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividiendo la data en entrenamiento y pruebas\n",
    "\n",
    "Para entrenar el modelo y poder probarlo. se divide el dataset entre dos grupos: entrenamiento y pruebas. \n",
    "La librería scklearn nos permite hacer una división del dataset en los cuatro vectores necesarios: características de entrenamiento, resultado de entrenamiento, caracteristicas de pruebas y resultado de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de registros para entrenamiento es: 32778\n",
      "        academica_0  academica_1  academica_2  academica_3  academica_4  \\\n",
      "148236            0            0            0            1            0   \n",
      "149863            0            0            0            1            0   \n",
      "\n",
      "        academica_5  personal_0  personal_1  personal_2  personal_3  ...  \\\n",
      "148236            0           0           1           0           0  ...   \n",
      "149863            0           0           1           0           0  ...   \n",
      "\n",
      "        institucional_1  institucional_2  institucional_3  institucional_4  \\\n",
      "148236                0                0                1                0   \n",
      "149863                0                0                1                0   \n",
      "\n",
      "        economica_0  economica_1  economica_2  economica_3  economica_4  \\\n",
      "148236            0            0            1            0            0   \n",
      "149863            1            0            0            0            0   \n",
      "\n",
      "        economica_5  \n",
      "148236            0  \n",
      "149863            0  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "El número de objetivos para entrenamiento es: 32778\n",
      "148236    0\n",
      "149863    0\n",
      "Name: deserta, dtype: int64\n",
      "El número de registros para prueba es: 8195\n",
      "        academica_0  academica_1  academica_2  academica_3  academica_4  \\\n",
      "39503             0            0            1            0            0   \n",
      "160493            0            0            1            0            0   \n",
      "\n",
      "        academica_5  personal_0  personal_1  personal_2  personal_3  ...  \\\n",
      "39503             0           0           1           0           0  ...   \n",
      "160493            0           0           1           0           0  ...   \n",
      "\n",
      "        institucional_1  institucional_2  institucional_3  institucional_4  \\\n",
      "39503                 0                1                0                0   \n",
      "160493                0                1                0                0   \n",
      "\n",
      "        economica_0  economica_1  economica_2  economica_3  economica_4  \\\n",
      "39503             0            0            1            0            0   \n",
      "160493            0            0            1            0            0   \n",
      "\n",
      "        economica_5  \n",
      "39503             0  \n",
      "160493            0  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "El número de registros para prueba es: 8195\n",
      "39503     0\n",
      "160493    0\n",
      "Name: deserta, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "targets_total = processed_data['deserta']\n",
    "features_total = processed_data.drop('deserta', axis=1)\n",
    "features, features_test, targets, targets_test = train_test_split( features_total, targets_total, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"El número de registros para entrenamiento es:\", len(features))\n",
    "print(features[:2])\n",
    "print(\"El número de objetivos para entrenamiento es:\", len(targets))\n",
    "print(targets[:2])\n",
    "\n",
    "print(\"El número de registros para prueba es:\", len(features_test))\n",
    "print(features_test[:2])\n",
    "print(\"El número de registros para prueba es:\", len(targets_test))\n",
    "print(targets_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar una red neuronal de una capa\n",
    "Para evaluar usando una red neuronal de una capa se generarán unas funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagación del error\n",
    "\n",
    "Creación de una función para determinar la propagación de los errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_term_formula(x, y, output):\n",
    "     return (y - output)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "En este caso el Adaline nos generará unos pesos como resultado del entrenamiento, que son lo que se irán ajustando para dar el resultado esperado en una regresión linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciclo: 0\n",
      "Pérdida de entrenamiento:  0.6676256299347455\n",
      "=========\n",
      "Ciclo: 50\n",
      "Pérdida de entrenamiento:  0.6484325334620592\n",
      "=========\n",
      "Ciclo: 100\n",
      "Pérdida de entrenamiento:  0.647301597925497\n",
      "=========\n",
      "Ciclo: 150\n",
      "Pérdida de entrenamiento:  0.6467065512699041\n",
      "=========\n",
      "Ciclo: 200\n",
      "Pérdida de entrenamiento:  0.6463553889383059\n",
      "=========\n",
      "Ciclo: 250\n",
      "Pérdida de entrenamiento:  0.6461304173180263\n",
      "=========\n",
      "Ciclo: 300\n",
      "Pérdida de entrenamiento:  0.6459763340782728\n",
      "=========\n",
      "Ciclo: 350\n",
      "Pérdida de entrenamiento:  0.6458649357369747\n",
      "=========\n",
      "Ciclo: 400\n",
      "Pérdida de entrenamiento:  0.6457808613911057\n",
      "=========\n",
      "Ciclo: 450\n",
      "Pérdida de entrenamiento:  0.6457152383732087\n",
      "=========\n",
      "Entrenamiento finalizado!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparametros del modelo, se pueden ajustar para encontrar los que generan mayor tasa de exito\n",
    "# Iteraciones en la red\n",
    "epochs = 500\n",
    "# Indice de aprendizaje, es un valor entre 0 y 1\n",
    "learnrate = 0.00001\n",
    "\n",
    "# Funcion de entrenamiento\n",
    "def train_nn(features, targets, epochs, learnrate):\n",
    "    \n",
    "    # generación de semilla aleatoria\n",
    "    np.random.seed(42)\n",
    "\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "\n",
    "    # Se inicializan los pesos\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(features.values, targets):\n",
    "            # Recorre los registros, x es la entrada, y el objetivo\n",
    "\n",
    "            # Activación de la unidad de salida \n",
    "            output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "            # Determinar el término de error\n",
    "            error_term = error_term_formula(x, y, output)\n",
    "\n",
    "            # El paso descendiente del gradiente, la sumatoria del error\n",
    "            del_w += error_term\n",
    "\n",
    "        # Se actualizan los pesos\n",
    "        weights += learnrate * del_w\n",
    "\n",
    "        # Se imprime la media del cuadrado del error en el set de entrenamiento\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean(error_formula(targets, out))\n",
    "            print(\"Ciclo:\", e)\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Pérdida de entrenamiento: \", loss, \" ALERTA - Pérdida de entrenamiento incrementado /!\\\\\")\n",
    "            else:\n",
    "                print(\"Pérdida de entrenamiento: \", loss)\n",
    "            last_loss = loss\n",
    "            print(\"=========\")\n",
    "    print(\"Entrenamiento finalizado!\")\n",
    "    return weights\n",
    "    \n",
    "weights = train_nn(features, targets, epochs, learnrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando la precisión del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.650\n"
     ]
    }
   ],
   "source": [
    "test_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = test_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Precisión: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando el modelo entrenado\n",
    "Una vez que se han ajustado los hiperparámetros, entrenado el modelo y determinado que se tiene una precisión adecuada, se puede salvar el modelo para no volver a entrenarlo de nuevo y sólo ejecutar el paso anterior de prueba o la ejecución real para obtener la regresión que arroja el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modeloAdaline.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = \"modeloAdaline.joblib\"\n",
    "joblib.dump(weights, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando el modelo\n",
    "Si desea cargar un modelo previamente entrenado se puede ejecutar el siguiente código. Nótese que se vuelve a cargar la librería y se define de nuevo el nombre del archivo, en este caso, por si el bloque de código anterior se ejecutó en una sesión diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "filename = \"modeloAdaline.joblib\"\n",
    "weights = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Usando el Perceptrón Multicapa de scikit\n",
    "\n",
    "Para el uso del perceptrón multicapa, se hace primero un escalamiento estandarizado, y luego se ejecuta un entrenamiento y prueba como con el Adaline de una sola capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19443238 -0.20232382 -1.48167463  1.92057273 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524 -1.03142904  1.27391895  0.         -0.38346639 -0.84597667\n",
      "   1.28728442 -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382 -1.48167463  1.92057273 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524 -1.03142904  1.27391895  0.          2.60779047 -0.84597667\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524 -1.03142904  1.27391895  0.         -0.38346639  1.1820657\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524  0.96952865 -0.7849793   0.         -0.38346639  1.1820657\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679 -1.86212071  1.94463859 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524 -1.03142904  1.27391895  0.         -0.38346639  1.1820657\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]]\n",
      "[[-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524  0.96952865 -0.7849793   0.         -0.38346639 -0.84597667\n",
      "   1.28728442 -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524  0.96952865 -0.7849793   0.         -0.38346639 -0.84597667\n",
      "   1.28728442 -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382 -1.48167463  1.92057273 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         19.96826751\n",
      "  -0.33475524 -1.03142904 -0.7849793   0.          2.60779047 -0.84597667\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679  0.53702211 -0.51423437 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524  0.96952865 -0.7849793   0.         -0.38346639  1.1820657\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]\n",
      " [-0.19443238 -0.20232382  0.67491201 -0.52067802 -0.15103062 -0.04024372\n",
      "  -0.10815679 -1.86212071  1.94463859 -0.05614496  0.         -0.05007946\n",
      "  -0.33475524  0.96952865 -0.7849793   0.          2.60779047 -0.84597667\n",
      "  -0.7768291  -0.28821692 -0.04024372  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(features)\n",
    "X_train_std = sc.transform(features)\n",
    "X_test_std = sc.transform(features_test)\n",
    "print(X_train_std[:2])\n",
    "print(X_test_std[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender la congiguración de los hiperparámetros se puede consultar:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76630094\n",
      "Iteration 2, loss = 0.76331306\n",
      "Iteration 3, loss = 0.76039933\n",
      "Iteration 4, loss = 0.75754929\n",
      "Iteration 5, loss = 0.75477086\n",
      "Iteration 6, loss = 0.75207293\n",
      "Iteration 7, loss = 0.74945390\n",
      "Iteration 8, loss = 0.74687905\n",
      "Iteration 9, loss = 0.74431893\n",
      "Iteration 10, loss = 0.74182454\n",
      "Iteration 11, loss = 0.73938857\n",
      "Iteration 12, loss = 0.73701755\n",
      "Iteration 13, loss = 0.73473436\n",
      "Iteration 14, loss = 0.73251449\n",
      "Iteration 15, loss = 0.73034652\n",
      "Iteration 16, loss = 0.72822712\n",
      "Iteration 17, loss = 0.72616973\n",
      "Iteration 18, loss = 0.72416503\n",
      "Iteration 19, loss = 0.72221089\n",
      "Iteration 20, loss = 0.72031619\n",
      "Iteration 21, loss = 0.71847006\n",
      "Iteration 22, loss = 0.71666768\n",
      "Iteration 23, loss = 0.71490685\n",
      "Iteration 24, loss = 0.71318835\n",
      "Iteration 25, loss = 0.71151199\n",
      "Iteration 26, loss = 0.70987973\n",
      "Iteration 27, loss = 0.70828546\n",
      "Iteration 28, loss = 0.70673664\n",
      "Iteration 29, loss = 0.70523892\n",
      "Iteration 30, loss = 0.70378357\n",
      "Iteration 31, loss = 0.70236721\n",
      "Iteration 32, loss = 0.70098197\n",
      "Iteration 33, loss = 0.69963144\n",
      "Iteration 34, loss = 0.69831542\n",
      "Iteration 35, loss = 0.69703205\n",
      "Iteration 36, loss = 0.69578022\n",
      "Iteration 37, loss = 0.69456473\n",
      "Iteration 38, loss = 0.69337807\n",
      "Iteration 39, loss = 0.69222479\n",
      "Iteration 40, loss = 0.69110202\n",
      "Iteration 41, loss = 0.69000845\n",
      "Iteration 42, loss = 0.68894327\n",
      "Iteration 43, loss = 0.68790673\n",
      "Iteration 44, loss = 0.68689816\n",
      "Iteration 45, loss = 0.68591459\n",
      "Iteration 46, loss = 0.68495615\n",
      "Iteration 47, loss = 0.68402264\n",
      "Iteration 48, loss = 0.68311546\n",
      "Iteration 49, loss = 0.68223014\n",
      "Iteration 50, loss = 0.68136934\n",
      "Iteration 51, loss = 0.68053030\n",
      "Iteration 52, loss = 0.67971390\n",
      "Iteration 53, loss = 0.67892324\n",
      "Iteration 54, loss = 0.67815285\n",
      "Iteration 55, loss = 0.67740634\n",
      "Iteration 56, loss = 0.67667649\n",
      "Iteration 57, loss = 0.67596732\n",
      "Iteration 58, loss = 0.67527471\n",
      "Iteration 59, loss = 0.67460113\n",
      "Iteration 60, loss = 0.67394652\n",
      "Iteration 61, loss = 0.67331080\n",
      "Iteration 62, loss = 0.67268999\n",
      "Iteration 63, loss = 0.67208441\n",
      "Iteration 64, loss = 0.67149738\n",
      "Iteration 65, loss = 0.67092392\n",
      "Iteration 66, loss = 0.67036374\n",
      "Iteration 67, loss = 0.66981855\n",
      "Iteration 68, loss = 0.66928614\n",
      "Iteration 69, loss = 0.66877004\n",
      "Iteration 70, loss = 0.66826443\n",
      "Iteration 71, loss = 0.66777153\n",
      "Iteration 72, loss = 0.66729256\n",
      "Iteration 73, loss = 0.66682964\n",
      "Iteration 74, loss = 0.66637863\n",
      "Iteration 75, loss = 0.66593803\n",
      "Iteration 76, loss = 0.66551066\n",
      "Iteration 77, loss = 0.66509229\n",
      "Iteration 78, loss = 0.66468580\n",
      "Iteration 79, loss = 0.66428892\n",
      "Iteration 80, loss = 0.66390244\n",
      "Iteration 81, loss = 0.66352568\n",
      "Iteration 82, loss = 0.66315751\n",
      "Iteration 83, loss = 0.66280251\n",
      "Iteration 84, loss = 0.66245562\n",
      "Iteration 85, loss = 0.66211759\n",
      "Iteration 86, loss = 0.66178892\n",
      "Iteration 87, loss = 0.66146786\n",
      "Iteration 88, loss = 0.66115523\n",
      "Iteration 89, loss = 0.66085003\n",
      "Iteration 90, loss = 0.66055230\n",
      "Iteration 91, loss = 0.66026101\n",
      "Iteration 92, loss = 0.65997662\n",
      "Iteration 93, loss = 0.65969831\n",
      "Iteration 94, loss = 0.65942765\n",
      "Iteration 95, loss = 0.65916120\n",
      "Iteration 96, loss = 0.65889961\n",
      "Iteration 97, loss = 0.65864310\n",
      "Iteration 98, loss = 0.65839603\n",
      "Iteration 99, loss = 0.65815472\n",
      "Iteration 100, loss = 0.65791960\n",
      "Precisión: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Indice de aprendizaje, es un valor entre 0 y 1\n",
    "learnrate_mlp = 0.00001\n",
    "#Número de iteraciones, default 200\n",
    "iteraciones = 100\n",
    "#solver_alg -> algoritmo de regresión, posibles valores: ‘lbfgs’, ‘sgd’, ‘adam’\n",
    "#‘lbfgs’ es un optimizador de la familia de métodos quasi-Newton.\n",
    "#‘sgd’ se refiere a stochastic gradient descent.\n",
    "#‘adam’ se refiere a un optimizador estocástico basado en gradientes, porpuesto por Kingma, Diederik y Jimmy Ba. Por defecto\n",
    "solver_alg = 'adam'\n",
    "#Número de neuronas por capa\n",
    "neuronas=10\n",
    "#Tipo de aprendizaje de tipo ‘constant’, ‘invscaling’, ‘adaptive’\n",
    "tipo_aprendizaje = 'adaptive'\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver=solver_alg, max_iter=iteraciones,  verbose= True, \n",
    "                    learning_rate_init=learnrate_mlp, hidden_layer_sizes=(neuronas, ), \n",
    "                    learning_rate=tipo_aprendizaje,random_state=1)\n",
    "\n",
    "#Training  features\n",
    "clf.fit(X_train_std,targets)\n",
    "#Ejecutar para obtenerr las predicciones\n",
    "predictions = clf.predict(X_test_std)\n",
    "\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Precisión: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sólo ejecutar \n",
    "Sólo se ejecuta el modelo para obtener las predicciones, se exporta en un archivo separado por comas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_std)\n",
    "\n",
    "import  csv\n",
    "with open(\"predicciones_PerceptronMulticapa.csv\",\"w\") as file:\n",
    "    wr = csv.writer(file,delimiter=\",\")\n",
    "    wr.writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar el modelo\n",
    "Puede cambiar el nombre del archivo para salvar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modeloPerceptronMulticapa.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = \"modeloPerceptronMulticapa.joblib\"\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo y el dataset para ejecutarlo\n",
    "Se puede cargar un dataset específico y el modelo entrenado para obtener los resultados de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga el modelo\n",
    "import joblib\n",
    "filename = \"modeloPerceptronMulticapa.joblib\"\n",
    "clf = joblib.load(filename)\n",
    "#Carga los datos, en este caso cargamos un archivo csv de ejemplo, pero se puede cargar data real para procesar\n",
    "sample_data = pd.read_csv(\"sample_data.csv\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(sample_data)\n",
    "X_train_std = sc.transform(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando el Perceptron regresor multicapa de scikit\n",
    "\n",
    "Se pueden configurar varios hiperparámetros, para ver más opciones, se puede consultar:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html?highlight=mlpregressor#sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.18552706\n",
      "Iteration 2, loss = 0.15235778\n",
      "Iteration 3, loss = 0.14243162\n",
      "Iteration 4, loss = 0.13633295\n",
      "Iteration 5, loss = 0.13223013\n",
      "Iteration 6, loss = 0.12933920\n",
      "Iteration 7, loss = 0.12727497\n",
      "Iteration 8, loss = 0.12565179\n",
      "Iteration 9, loss = 0.12422946\n",
      "Iteration 10, loss = 0.12308335\n",
      "Iteration 11, loss = 0.12215953\n",
      "Iteration 12, loss = 0.12139975\n",
      "Iteration 13, loss = 0.12076637\n",
      "Iteration 14, loss = 0.12022010\n",
      "Iteration 15, loss = 0.11975263\n",
      "Iteration 16, loss = 0.11934584\n",
      "Iteration 17, loss = 0.11898779\n",
      "Iteration 18, loss = 0.11867242\n",
      "Iteration 19, loss = 0.11839226\n",
      "Iteration 20, loss = 0.11813179\n",
      "Iteration 21, loss = 0.11787950\n",
      "Iteration 22, loss = 0.11765238\n",
      "Iteration 23, loss = 0.11744086\n",
      "Iteration 24, loss = 0.11723690\n",
      "Iteration 25, loss = 0.11705828\n",
      "Iteration 26, loss = 0.11689725\n",
      "Iteration 27, loss = 0.11674906\n",
      "Iteration 28, loss = 0.11661563\n",
      "Iteration 29, loss = 0.11649177\n",
      "Iteration 30, loss = 0.11638239\n",
      "Iteration 31, loss = 0.11627977\n",
      "Iteration 32, loss = 0.11618651\n",
      "Iteration 33, loss = 0.11609719\n",
      "Iteration 34, loss = 0.11601097\n",
      "Iteration 35, loss = 0.11593289\n",
      "Iteration 36, loss = 0.11586150\n",
      "Iteration 37, loss = 0.11579002\n",
      "Iteration 38, loss = 0.11572624\n",
      "Iteration 39, loss = 0.11565779\n",
      "Iteration 40, loss = 0.11560304\n",
      "Iteration 41, loss = 0.11554301\n",
      "Iteration 42, loss = 0.11548777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 43, loss = 0.11545368\n",
      "Iteration 44, loss = 0.11544125\n",
      "Iteration 45, loss = 0.11543150\n",
      "Iteration 46, loss = 0.11542097\n",
      "Iteration 47, loss = 0.11541011\n",
      "Iteration 48, loss = 0.11540083\n",
      "Iteration 49, loss = 0.11539049\n",
      "Iteration 50, loss = 0.11538135\n",
      "Iteration 51, loss = 0.11537201\n",
      "Iteration 52, loss = 0.11536411\n",
      "Iteration 53, loss = 0.11535538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 54, loss = 0.11534900\n",
      "Iteration 55, loss = 0.11534742\n",
      "Iteration 56, loss = 0.11534586\n",
      "Iteration 57, loss = 0.11534430\n",
      "Iteration 58, loss = 0.11534256\n",
      "Iteration 59, loss = 0.11534107\n",
      "Iteration 60, loss = 0.11533929\n",
      "Iteration 61, loss = 0.11533776\n",
      "Iteration 62, loss = 0.11533614\n",
      "Iteration 63, loss = 0.11533447\n",
      "Iteration 64, loss = 0.11533301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 65, loss = 0.11533168\n",
      "Iteration 66, loss = 0.11533136\n",
      "Iteration 67, loss = 0.11533103\n",
      "Iteration 68, loss = 0.11533071\n",
      "Iteration 69, loss = 0.11533045\n",
      "Iteration 70, loss = 0.11533011\n",
      "Iteration 71, loss = 0.11532975\n",
      "Iteration 72, loss = 0.11532945\n",
      "Iteration 73, loss = 0.11532913\n",
      "Iteration 74, loss = 0.11532881\n",
      "Iteration 75, loss = 0.11532849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 76, loss = 0.11532823\n",
      "Iteration 77, loss = 0.11532816\n",
      "Iteration 78, loss = 0.11532810\n",
      "Iteration 79, loss = 0.11532804\n",
      "Iteration 80, loss = 0.11532797\n",
      "Iteration 81, loss = 0.11532791\n",
      "Iteration 82, loss = 0.11532784\n",
      "Iteration 83, loss = 0.11532779\n",
      "Iteration 84, loss = 0.11532772\n",
      "Iteration 85, loss = 0.11532765\n",
      "Iteration 86, loss = 0.11532759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Prediction accuracy: 0.647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "#Rating de aprendizaje\n",
    "learnrate_MLPR = 0.0004\n",
    "#Iteraciones\n",
    "epochs_MLPR=1000\n",
    "#Número de neuronas\n",
    "neuronas = 10\n",
    "#Solver posibles valores 'lbfgs', 'sgd', 'adam'\n",
    "solver_alg = 'sgd'\n",
    "#Tipo de aprendizaje valores posibles 'constant', 'invscaling', 'adaptive'\n",
    "tipo_aprendizaje= 'adaptive'\n",
    "\n",
    "regr = MLPRegressor( max_iter=epochs_MLPR, \n",
    "                    learning_rate_init=learnrate_MLPR, solver=solver_alg, \n",
    "                    hidden_layer_sizes=(neuronas, ), learning_rate=tipo_aprendizaje, \n",
    "                    random_state=1, verbose= True).fit(features, targets)\n",
    "predictions = regr.predict(features_test)\n",
    "predictions = np.round(predictions, 0).astype(int)\n",
    "\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sólo ejecutar \n",
    "Sólo se ejecuta el modelo para obtener las predicciones, se exporta en un archivo separado por comas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regr.predict(features_test)\n",
    "import  csv\n",
    "with open(\"predicciones_PerceptronMulticapaRegresor.csv\",\"w\") as file:\n",
    "    wr = csv.writer(file,delimiter=\",\")\n",
    "    wr.writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar el modelo\n",
    "Puede cambiar el nombre del archivo para salvar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modeloPerceptronMulticapaRegresor.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = \"modeloPerceptronMulticapaRegresor.joblib\"\n",
    "joblib.dump(regr, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo y el dataset para ejecutarlo\n",
    "Se puede cargar un dataset específico y el modelo entrenado para obtener los resultados de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga el modelo\n",
    "import joblib\n",
    "filename = \"modeloPerceptronMulticapaRegresor.joblib\"\n",
    "regr = joblib.load(filename)\n",
    "#Carga los datos, en este caso cargamos un archivo csv de ejemplo, pero se puede cargar data real para procesar\n",
    "features_test = pd.read_csv(\"sample_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando el Regresor Random Forest de scikit\n",
    "PAra entender la configuración, se puede ver esta página:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforestregressor#sklearn.ensemble.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "Prediction accuracy: 0.648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "arboles=100\n",
    "rf = RandomForestRegressor(n_estimators = arboles, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(features, targets);\n",
    "predictions = rf.predict(features_test)\n",
    "predictions = np.round(predictions, 0).astype(int)\n",
    "print(predictions[:10])\n",
    "\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sólo ejecutar \n",
    "Sólo se ejecuta el modelo para obtener las predicciones, se exporta en un archivo separado por comas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(features_test)\n",
    "import  csv\n",
    "with open(\"predicciones_RegresorRandomForest.csv\",\"w\") as file:\n",
    "    wr = csv.writer(file,delimiter=\",\")\n",
    "    wr.writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar el modelo\n",
    "Puede cambiar el nombre del archivo para salvar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modeloRegresorRandomForest.joblib']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = \"modeloRegresorRandomForest.joblib\"\n",
    "joblib.dump(rf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo y el dataset para ejecutarlo\n",
    "Se puede cargar un dataset específico y el modelo entrenado para obtener los resultados de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga el modelo\n",
    "import joblib\n",
    "filename = \"modeloRegresorRandomForest.joblib\"\n",
    "rf = joblib.load(filename)\n",
    "#Carga los datos, en este caso cargamos un archivo csv de ejemplo, pero se puede cargar data real para procesar\n",
    "features_test = pd.read_csv(\"sample_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
